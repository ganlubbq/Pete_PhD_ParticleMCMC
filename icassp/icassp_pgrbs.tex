\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% MY PACKAGES %%%

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{IEEEtrantools}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage[caption=false]{subfig}
\usepackage{setspace}

\usepackage{tikz}
\usepackage{pgfplots}
 \usetikzlibrary{plotmarks}
 \pgfplotsset{compat=newest}
 \pgfplotsset{plot coordinates/math parser=false}
 \usepgfplotslibrary{external}
 \tikzexternalize[prefix=tikz/]
 
\setlength{\emergencystretch}{20pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% MACROS %%%
\newcommand{\ti}{t}
\newcommand{\timax}{T}

\newcommand{\pr}{\theta}
\newcommand{\prspace}{\Theta}

\newcommand{\ls}[1]{x_{#1}}
\newcommand{\lsspace}{\mathcal{X}}

\newcommand{\ob}[1]{y_{#1}}
\newcommand{\obspace}{\mathcal{Y}}

\newcommand{\nc}{Z}

\newcommand{\toas}{\stackrel{\text{a.s.}}{\to}}
\newcommand{\testfunc}{\zeta}
\newcommand{\prob}{P}

\newcommand{\id}[1]{q_{#1}}

\newcommand{\an}[1]{a_{#1}}
\newcommand{\ai}[1]{b_{#1}}
\newcommand{\notai}[1]{-b_{#1}}
\newcommand{\aifinal}{K}
\newcommand{\lsset}[1]{\mathbf{x}_{#1}}
\newcommand{\anset}[1]{\mathbf{a}_{#1}}

\newcommand{\den}{p}
\newcommand{\ed}{\pi}
\newcommand{\td}[1]{f_{\theta,#1}}
\newcommand{\od}[1]{g_{\theta,#1}}
\newcommand{\pd}[1]{\phi_{#1}}
\newcommand{\spd}[1]{\psi_{#1}}

\newcommand{\pw}[1]{w_{#1}}
\newcommand{\ppw}[1]{v_{#1}}
\newcommand{\spw}[1]{u_{#1}}

\newcommand{\mhap}{\alpha}
\newcommand{\pss}[1]{^{(#1)}}
\newcommand{\nump}{N}
\newcommand{\utf}[1]{\rho_{#1}}
\newcommand{\cised}{\eta}
\newcommand{\cisi}{c}
\newcommand{\notcisi}{-c}

\newcommand{\wl}{L}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Particle Gibbs with Refreshed Backward Simulation}
% \name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
\name{Pete Bunch \qquad Fredrik Lindsten \qquad Sumeetpal Singh}
\address{Signal Processing and Communications Laboratory\\
         Department of Engineering, University of Cambridge\\
         Trumpington Street, Cambridge, UK, CB2 1PZ}

\begin{document}
\ninept
\maketitle

\begin{abstract}
 The particle Gibbs algorithm can be used for Bayesian parameter estimation in Markovian state space models. Sometimes the resulting Markov chains mix slowly when the component particle filter suffers from degeneracy. This effect can be somewhat alleviated using backward simulation. In this paper we show how a simple modification to this scheme, which we refer to as refreshed backward simulation, can further improve the mixing. This works by sampling new state values simultaneously with the corresponding ancestor indexes. Although the necessary conditional distributions cannot be sampled directly, we provide suitable Markov kernels which target them. The efficacy of this new scheme is demonstrated with a simulation example.
\end{abstract}
%
\begin{keywords}
Sequential Monte Carlo, particle Markov chain Monte Carlo, Gibbs sampling, backward simulation
\end{keywords}


\section{Introduction}
Particle Markov chain Monte Carlo (PMCMC) algorithms \cite{Andrieu2010,Olsson2011,Chopin2014,Lindsten2014} provide an elegant and effective solution for Bayesian parameter learning with Markovian state space models. They are based on the formulation of an extended target distribution over the system of random variables comprising a particle filter, which has the desired posterior distribution as a marginal. A Markov chain for which this extended distribution is invariant may be constructed using sequential Monte Carlo (SMC) components. The particle system is composed of a set of states for each time step and corresponding ancestor index variables, which define a set of state trajectories.

In this paper we consider in particular the \emph{particle Gibbs} (PG) algorithm, introduced by \cite{Andrieu2010}. This samples in turn new values for the unknown parameters, the particle system, and an index variable indicating one reference trajectory. Sampling the particle system is equivalent to running a modified particle filter. Like any Gibbs sampler, this has the advantage over Metropolis-Hastings of not requiring an accept/reject stage. However, the resulting chains are still liable to mix slowly if the particle filter suffers from path-space degeneracy.

It is possible to reduce degeneracy, and thus improve the mixing of the PG Markov chain, by incorporating additional sampling steps, either during the filtering stage, known as particle Gibbs with ancestor sampling (PG-AS) \cite{Lindsten2014}, or in an additional backward sweep, known as particle Gibbs with backward simulation (PG-BS) \cite{Whiteley2010b,Lindsten2012}. The improvement arises from sampling new values for individual ancestor indexes, and thus allowing the reference trajectory to be updated gradually rather than changing it all at once.

For near-degenerate models, mixing may be slow even when using PG-BS or PG-AS. Specifically, when the model transition distribution is highly informative, the probability of sampling any change in the particle ancestry is low. Intuitively, the problem is that the only state history consistent with a particular future is that from which the future was originally generated. We can mitigate this effect by using a modified procedure which we call \emph{refreshed backward simulation} \cite{Bunch2012,Bunch2014}. When sampling an ancestor index for the reference trajectory, we simultaneously sample a new value for the associated state. This allows us some leeway to steer the potential state histories towards the fixed future, consequently increasing the probability of changing the ancestry and thus improving the mixing of the Markov chain.

A recent paper \cite{Carter2014} presents ideas which have some overlap with our work. Specifically, they use Markov chain Monte Carlo during a backward sweep to sample new state values. However, they do not simultaneously sample the corresponding ancestor variable, as we do here, a step which is important in order to improve the mixing for near-degenerate models. Furthermore, they introduce a different extended target distribution in order to justify this addition, which necessitates changes to the conditional particle filter, whereas we only use the standard PMCMC distribution.



\subsection{State Space Modelling and the Particle Filter}
We consider a standard Markovian state space model with a sequence of latent states $\ls{\ti} \in \lsspace : \ti = 1,\dots,\timax$, and a corresponding sequence of observations $\ob{\ti} \in \obspace : \ti = 1,\dots,\timax$. We assume that the transition and observation distributions have associated densities with respect to some appropriate measure (e.g. Lebesgue),
%
\begin{IEEEeqnarray}{rClcrCl}
 \ls{\ti}|\ls{\ti-1} & \sim & \td{\ti}(\ls{\ti}|\ls{\ti-1}) & \qquad & \ob{\ti}|\ls{\ti}   & \sim & \od{\ti}(\ob{\ti}|\ls{\ti})   \nonumber       .
\end{IEEEeqnarray}
%
We use the convention that $\td{1}(\ls{1}|\ls{0})=\td{1}(\ls{1})$ is the prior density of the first state. The variable $\pr \in \prspace$ is a collection of unknown model parameters upon which $\td{\ti}$ and $\od{\ti}$ depend, which has a prior density $\den(\pr)$.

Our objective is to approximate the joint posterior density over all the unknown variables,
%
\begin{IEEEeqnarray}{rCl}
 \den(\pr, \ls{1:\timax} | \ob{1:\timax}) & = & \frac{1}{\nc} \den(\pr) \prod_{\ti=1}^{\timax} \od{\ti}(\ob{\ti}|\ls{\ti}) \td{\ti}(\ls{\ti}|\ls{\ti-1}) \label{eq:full-posterior}      ,
\end{IEEEeqnarray}
%
where $\nc$ is a normalising constant.

The particle filter is a sequential Monte Carlo algorithm which recursively approximates the sequence of densities $\den(\ls{1:\ti}|\pr,\ob{1:\ti}) \allowbreak : \ti = 1,\dots,\timax$. This is achieved by propagating forwards a collection of $\nump$ particles $\{\ls{1:\ti}\pss{i}: i = 1,\dots,\nump\}$, each of which is a realisation of the state sequence, along with a set of associated weights $\{\pw{\ti}\pss{i}: i = 1,\dots,\nump\}$. This procedure is well established. See e.g. \cite{Cappe2007,Doucet2009,Andrieu2010,Lindsten2012} for details of particle filters and their use as a component in PMCMC schemes.

Particle filters exhibit a significant deficiency known as path-space degeneracy. Only a subset of the particles at each time instant are used in the construction of those at the next time instant. This means that the number of unique states appearing in the trajectories decreases as we look back in time. If $\timax$ is sufficiently large, then there will be a time step before which every particle is the same \cite{Jacob2013}.



\section{Particle Gibbs}
An ideal Gibbs sampler for targeting \eqref{eq:full-posterior} might alternately sample from the state and parameter conditional distributions, $\den(\ls{1:\timax}|\pr,\ob{1:\timax})$ and $\den(\pr|\ls{1:\timax},\ob{1:\timax})$. We assume here that the parameter conditional is straightforward to sample from, either directly using conjugate priors or with Metropolis-Hastings (MH). Sampling from the state conditional is the more challenging step. This can rarely be achieved directly. A particle filter could be used to return an approximately distributed sample, but the resulting algorithm will not have the correct target distribution because of this approximation. The approach used by particle Gibbs is to construct an extended distribution over all the random variables comprising a particle filter. This may be targeted without approximation, and admits the desired posterior as a marginal.

The PMCMC extended target distribution is constructed over the space of an entire particle system. This comprises states and ancestor indexes,
%
\begin{IEEEeqnarray}{rClCl}
 \lsset{\ti} = \{\ls{\ti}\pss{i} : i = 1,\dots,\nump\} & \quad & \ti = 1,\dots,\timax \nonumber \\
 \anset{\ti} = \{\an{\ti}\pss{i} : i = 1,\dots,\nump\} & \quad & \ti = 2,\dots,\timax \nonumber      .
\end{IEEEeqnarray}
%
The ancestor index $\an{\ti}\pss{i} \in \{1,\dots,\nump\}$ indicates the $(\ti-1)$ parent state from which $\ls{\ti}\pss{i}$ follows. Hence, each particle state trajectory is constructed recursively by tracing the lineage described by these ancestor indexes,
%
\begin{IEEEeqnarray}{rCl}
 \ls{1:\ti}\pss{i} & = & \ls{1:\ti-1}\pss{\an{\ti}\pss{i}} \cup \ls{\ti}\pss{i} \nonumber     .
\end{IEEEeqnarray}
%
Furthermore, let $\aifinal\in\{1,\dots,\nump\}$ be the index of one particular reference trajectory, and indicate the ancestry of this particle by,
%
\begin{IEEEeqnarray}{rCl}
 \ai{\ti} &=& \begin{cases}
               \aifinal & \ti = \timax \\
               \an{\ti+1}\pss{\ai{\ti+1}} & \ti = \timax-1,\dots,1     .
              \end{cases} \nonumber
\end{IEEEeqnarray}
%
For this reference trajectory write,
%
\begin{IEEEeqnarray}{rCl}
 \ls{1:\timax}\pss{\ai{1:\timax}} = \{ \ls{\ti}\pss{\ai{\ti}} : \ti = 1,\dots,\timax \} \nonumber     ,
\end{IEEEeqnarray}
%
and for the remaining states,
%
\begin{IEEEeqnarray}{rCl}
 \ls{1:\timax}\pss{\notai{1:\timax}} = \lsset{1:\timax} \setminus \ls{1:\timax}\pss{\ai{1:\timax}} \nonumber     .
\end{IEEEeqnarray}
%
The extended target distribution may now be written as,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \ed(\pr, \anset{2:\timax}, \lsset{1:\timax}, \aifinal) = \frac{1}{\nump^\timax} \den(\pr, \ls{1:\timax}\pss{\ai{1:\timax}}|\ob{1:\timax}) }\nonumber \\
  \qquad \times \prod_{i\ne\ai{1}} \id{1}(\ls{1}\pss{i}) \prod_{\ti=2}^{\timax} \left[ \prod_{i\ne\ai{\ti}} \frac{ \pw{\ti-1}\pss{\an{\ti}\pss{i}} }{ \sum_j \pw{\ti-1}\pss{j} } \id{\ti}(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}}) \right] \label{eq:extended_dist_v1}     ,
\end{IEEEeqnarray}
%
in which the unnormalised importance weights are,
%
\begin{IEEEeqnarray}{rCl}
 \pw{\ti}\pss{i} = \td{\ti}(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}})\od{\ti}(\ob{\ti}|\ls{\ti}\pss{i}) / \id{\ti}(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}})
\end{IEEEeqnarray}
%
and $\{\id{\ti}\}$ are importance densities. These may depend on the observation sequence $\ob{1:\timax}$, and the same convention regarding $\id{1}$ is used as for the $\td{1}$. By construction the extended target distribution has the desired posterior as a marginal.

During each step of the particle Gibbs algorithm new values are sampled in turn from appropriate conditional posterior distributions for $\pr$, then $\{\anset{2:\timax}\pss{\notai{2:\timax}}, \lsset{1:\timax}\pss{\notai{1:\timax}}\}$, and finally $\aifinal$.

The conditional for the non-reference particles may, by construction, be sampled sequentially forwards in time. This procedure is known as a \emph{conditional particle filter}, since it consists of the same operations as a standard particle filter, but for one ancestor-state pair at each time step which is set deterministically to be equal to that of the reference trajectory.

The conditional for the reference trajectory index is \cite{Andrieu2010},
%
\begin{IEEEeqnarray}{rCl}
 \ed(\aifinal|\pr, \anset{2:\timax}, \lsset{1:\timax}) &=& \frac{\pw{\timax}\pss{\aifinal}}{\sum_j \pw{\timax}\pss{j}} \nonumber      .
\end{IEEEeqnarray}
%
Thus, an index is sampled by normalising the final particle filter weights and then drawing once from the resulting categorical distribution.


\section{Particle Gibbs with Backward Simulation}
Mixing of the particle Gibbs algorithm can be very slow. This can be seen as a failing of the conditional particle filter. The reference trajectory is guaranteed to appear in the final particle system. If the system suffers from path-space degeneracy then the old and new reference trajectories are likely to have a near-identical ancestry, with differences only appearing towards the end of the sequence. The Markov chain required for the early states to converge will be impractically long.

\subsection{Standard Backward Simulation}
As suggested by \cite{Whiteley2010b}, this problem may be mitigated by including an additional sampling stage in each step of the PG algorithm. Sweeping backwards, for each time step a new ancestor index is drawn from,
%
\begin{IEEEeqnarray}{l}
 \ed(\an{\ti}\pss{\ai{\ti}} | \pr, \anset{2:\ti-1}, \lsset{1:\ti-1}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \ls{\ti:\timax}\pss{\ai{\ti:\timax}}, \aifinal) \nonumber \\
 \qquad = \frac{ \pw{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}} \td{\ti}(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}) }{ \sum_j \pw{\ti-1}\pss{j} \td{\ti}(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{j}) } \label{eq:bs-distn}     .
\end{IEEEeqnarray}
%
Note that each of these operations is a \emph{collapsed} Gibbs move, meaning that some of the variables are marginalised before conditioning. Specifically, we have marginalised the future states and ancestors other than those in the reference trajectory. Marginalisation is commonly used for Gibbs samplers to improve the mixing and to simplify the implementation, and if correctly formulated will not alter the stationary distribution; see \cite{Dyk2008} for details.

Algorithmically, this additional stage corresponds to backward simulation \cite{Godsill2004}. The sampler sweeps backwards through time, sampling a new value for each ancestor index $\an{\ti}\pss{{\ai{\ti}}}$ from a set of smoothing weights proportional to $\pw{\ti}\pss{i}\td{\ti}(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{i})$. Backward simulation within PG was suggested by \cite{Whiteley2010b}, and explored by \cite{Lindsten2012}, although in the latter case using a modified extended target distribution.


\subsection{Refreshed Backward Simulation}
Backward simulation allows the sampler to change the ancestry of the reference trajectory even when the conditional particle filter suffers from degeneracy. However, if the model transition density is tightly concentrated in one area of the state space (e.g. if the variance $\mathbb{V}[\ls{\ti}|\ls{\ti-1}]$ is small) then the probability of changing the ancestor indexes may be very low. See Fig~\ref{fig:bs-fail}. If this situation arises, then the ability of backward simulation to mitigate the problems of particle degeneracy and accelerate the mixing of PG can be limited.

\begin{figure}
 \centering
 \input{bs_fail_example.tikz}
 \caption{An illustration of ineffective backward simulation on a toy linear-Gaussian problem. Crosses indicate particle filter states, and dotted lines the ancestries. The reference particle is shown with a solid line. A backward simulation sweep does not result in any changes to the reference particle ancestry.}
 \label{fig:bs-fail}
\end{figure}

We can increase the chances of altering the ancestry, and thus further improve mixing of the Markov chain, if the backward simulation algorithm is modified to simultaneously sample a new value for each state along with the corresponding ancestor index. At each time instant we now sample from,
%
\begin{IEEEeqnarray}{l}
 \ed(\an{\ti}\pss{\ai{\ti}}, \ls{\ti}\pss{\ai{\ti}} | \pr, \anset{2:\ti-1}, \lsset{1:\ti-1}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \ls{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \aifinal) \label{eq:rbs-distn} \\
 = \frac{ \pw{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}} \td{\ti}(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}) \od{\ti}(\ob{\ti}|\ls{\ti}\pss{\ai{\ti}}) \td{\ti+1}(\ls{\ti+1}\pss{\ai{\ti+1}}|\ls{\ti}\pss{\ai{\ti}}) }{ \sum_j \pw{\ti-1}\pss{j} \int \td{\ti}(\ls{}|\ls{\ti-1}\pss{j}) \od{\ti}(\ob{\ti}|\ls{}) \td{\ti+1}(\ls{\ti+1}\pss{\ai{\ti+1}}|\ls{}) d\ls{} } \nonumber      .
\end{IEEEeqnarray}
%
As before, this is a collapsed Gibbs move.

In standard backward simulation, the conditional for each ancestor index is a categorical distribution \eqref{eq:bs-distn}, which can be sampled directly by evaluating the weight associated with each possible value, or by rejection sampling \cite{Douc2011,Taghavi2013}. It is also possible to use an MH kernel targeting this distribution \cite{Bunch2012}.

In contrast, the joint conditional for state-ancestor pairs is a mixed continuous-discrete distribution \eqref{eq:rbs-distn}. Since it will not in general be possible to sample from this distribution directly, we consider two possible Markov kernels which can be used instead. To clarify the following explanations, we write the one-step target distribution in a simplified form, omitting superfluous indexes and conditioning,
%
\begin{IEEEeqnarray}{rCl}
\ed(\an{\ti},\ls{\ti} | \lsset{\ti-1}) & = & \frac{ \pw{\ti-1}\pss{\an{\ti}} \utf{\ti}(\ls{\ti}|\ls{\ti-1}\pss{\an{\ti}}) }{ \sum_j \pw{\ti-1}\pss{j} \int \utf{\ti}(\ls{}|\ls{\ti-1}\pss{j}) d\ls{} } \label{eq:simplified-rbs-distn} \\
 \utf{\ti}(\ls{\ti}|\ls{\ti-1}\pss{\an{\ti}}) & = & \td{\ti}(\ls{\ti}|\ls{\ti-1}\pss{\an{\ti}}) \od{\ti}(\ob{\ti}|\ls{\ti}) \td{\ti+1}(\ls{\ti+1}\pss{\ai{\ti+1}}|\ls{\ti}) \nonumber      .
\end{IEEEeqnarray}

\subsubsection{Metropolis-Hastings}
We can target \eqref{eq:simplified-rbs-distn} using MH. From current values $\an{\ti}^*$ and $\ls{\ti}^*$, we can propose new values $\an{\ti}'$ and $\ls{\ti}'$ by drawing from,
%
\begin{IEEEeqnarray}{rCl}
 \frac{ \ppw{\ti-1}\pss{\an{\ti}} }{ \sum_j \ppw{\ti-1}\pss{j} } \pd{\ti}(\ls{\ti} | \ls{\ti-1}\pss{\an{\ti}}, \ls{\ti}^*) \label{eq:rbs-mh-ppsl}       ,
\end{IEEEeqnarray}
%
in which $\{\ppw{\ti-1}\pss{i} : i = 1,\dots,\nump\}$ are a set of proposal weights for the ancestor index and $\pd{\ti}$ is a new proposal density. The resulting acceptance probability is then,
%
\begin{IEEEeqnarray}{l}
 \mhap(\{\an{\ti}^*,\ls{\ti}^*\} \to \{\an{\ti}',\ls{\ti}'\}) \nonumber \\
 \quad = \min\left\{ 1, \frac{ \pw{\ti-1}\pss{\an{\ti}'} \utf{\ti}(\ls{\ti}'|\ls{\ti-1}\pss{\an{\ti}'}) }{ \ppw{\ti-1}\pss{\an{\ti}'} \pd{\ti}(\ls{\ti}' | \ls{\ti-1}\pss{\an{\ti}'}, \ls{\ti}^*) } \frac{ \ppw{\ti-1}\pss{\an{\ti}^*} \pd{\ti}(\ls{\ti}^* | \ls{\ti-1}\pss{\an{\ti}^*}, \ls{\ti}') }{ \pw{\ti-1}\pss{\an{\ti}^*} \utf{\ti}(\ls{\ti}^*|\ls{\ti-1}\pss{\an{\ti}^*}) } \right\}
\end{IEEEeqnarray}
%
This is the scheme suggested in \cite{Bunch2012} for state smoothing.


\subsubsection{Conditional Importance Sampling}
The marginal conditional distribution for the ancestor indexes is,
%
\begin{IEEEeqnarray}{rCl}
\ed(\an{\ti} | \lsset{\ti-1}) = \int \ed(\ls{}, \an{\ti} | \lsset{\ti-1}) d\ls{} & = & \frac{ \pw{\ti-1}\pss{\an{\ti}} \int \utf{\ti}(\ls{}|\ls{\ti-1}\pss{\an{\ti}})d\ls{} }{ \sum_j \pw{\ti-1}\pss{j} \int \utf{\ti}(\ls{}|\ls{\ti-1}\pss{j}) d\ls{} } \nonumber      .
\end{IEEEeqnarray}
%
If this distribution is dominated by a small number of ancestors with high probability, then an MH kernel will be inefficient. It may take a large number of steps before one of these is proposed. In such circumstances it may be advantageous to use the following method based on \emph{conditional importance sampling} (CIS) instead.

CIS uses the same principle as the conditional particle filter \cite{Andrieu2010}, but applied to a single time step. Suppose we have the current values $\an{\ti}^*$ and $\ls{\ti}^*$, then a Markov kernel may be constructed with \eqref{eq:simplified-rbs-distn} as its invariant distribution by following algorithm~\ref{alg:cis}, in which $\spd{\ti}(\ls{\ti}|\ls{\ti-1})$ is an appropriate importance density.

\begin{algorithm}[!h]
\begin{spacing}{1.5}
\begin{algorithmic}[1]
 \REQUIRE Preceding particle states $\lsset{\ti-1}$, current values $\an{\ti}^*$ and $\ls{\ti}^*$.
 \STATE Sample an index uniformly $\cisi^*\in\{1,\dots,\nump\}$.
 \STATE Set $\an{\ti}\pss{\cisi^*} = \an{\ti}^*$. Set $\ls{\ti}\pss{\cisi^*} = \ls{\ti}^*$.
 \FORALL{$i \in \{1,\dots,\nump\}\setminus\cisi^*$}
  \STATE Sample $\an{\ti}\pss{i} \sim \frac{\ppw{\ti-1}\pss{\an{\ti}}}{\sum_j \ppw{\ti-1}\pss{j}}$. Sample $\ls{\ti}\pss{i} \sim \spd{\ti}(\ls{\ti}|\ls{\ti-1}\pss{\an{\ti}\pss{i}})$.
 \ENDFOR
 \STATE Sample $\cisi' \sim \frac{\spw{\ti}\pss{\cisi}}{\sum_j \spw{\ti}\pss{j}}$, where $\spw{\ti} = \frac{ \pw{\ti-1}\pss{\an{\ti}\pss{i}} \utf{\ti}(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}}) }{ \ppw{\ti-1}\pss{\an{\ti}\pss{i}} \spd{\ti}(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}}) }$.
 \STATE Set $\an{\ti}' = \an{\ti}\pss{\cisi'}$.
 \STATE Set $\ls{\ti}' = \ls{\ti}\pss{\cisi'}$.
 \RETURN New values $\an{\ti}'$ and $\ls{\ti}'$.
\end{algorithmic}
\end{spacing}
\caption{Conditional importance sampling for the joint ancestor-state conditional distributions.}
\label{alg:cis}
\end{algorithm}

To justify that this is a correct Markov kernel, we construct another extended target distribution over these particles, (Note that this set of particles is separate to that of the primary Gibbs sampler.)
%
\begin{IEEEeqnarray}{rCl}
 \cised(\anset{\ti}, \lsset{\ti}, \cisi) & = & \frac{1}{\nump} \ed(\ls{\ti}\pss{\cisi}, \an{\ti}\pss{\cisi} | \lsset{\ti-1}) \prod_{i\ne\cisi} \frac{\ppw{\ti}\pss{\an{\ti}\pss{i}}}{\sum_j \ppw{\ti}\pss{j}} \spd{\ti}(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}}) \nonumber     . 
\end{IEEEeqnarray}
%
The first part of algorithm~\ref{alg:cis} by construction corresponds to sampling from the conditional distribution, $\cised(\anset{\ti}\pss{\notcisi}, \lsset{\ti}\pss{\notcisi} | \an{\ti}\pss{\cisi}, \ls{\ti}\pss{\cisi}, \cisi)$, and the final part to sampling from $\cised(\cisi|\anset{\ti}, \lsset{\ti})$. Hence, if the starting values are distributed according to the desired posterior, then the final values must also be, and so the procedure is a Markov kernel with the desired invariant distribution.

\section{Extensions and Variations}

\subsection{Multiple Time Steps}
In extreme cases, even with refreshed backward simulation the update rate of the earliest states may be low. If this occurs, it is may be beneficial to extend the method to sample states at multiple steps, thus giving us yet more leeway to match the sampled future to the possible particle histories. At each time instant we now sample from,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \ed(\an{\ti}\pss{\ai{\ti}}, \ls{\ti:\ti+\wl-1}\pss{\ai{\ti:\ti+\wl-1}} | \pr, \lsset{1:\ti-1}, \anset{2:\ti-1}, \ls{\ti+\wl:\timax}\pss{\ai{\ti+\wl:\timax}}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \aifinal) } \nonumber \\
 \qquad &\propto& \pw{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}} \td{\ti+\wl}(\ls{\ti+\wl}\pss{\ai{\ti+\wl}}|\ls{\ti+\wl-1}\pss{\ai{\ti+\wl-1}}) \nonumber \\
        &       & \qquad \times \prod_{k=\ti}^{\ti+\wl-1} \td{k}(\ls{k}\pss{\ai{k}}|\ls{k-1}\pss{\an{k}\pss{\ai{k}}}) \od{k}(\ob{k}|\ls{k}\pss{\ai{k}}) \nonumber      .
\end{IEEEeqnarray}
%
As before, this is a collapsed Gibbs move. Implementing such a scheme will require careful design of proposal or importance densities for the joining bridge of states $\ls{\ti:\ti+\wl-1}\pss{\ai{\ti:\ti+\wl-1}}$.


\subsection{Ancestor Sampling}
Rather than conducting the complete forward sweep (i.e. the conditional particle filter) followed by a backward simulation sweep, the steps of the two may be interleaved. This is the basis of particle Gibbs with ancestor sampling (PG-AS) \cite{Lindsten2014}. At time step $\ti$, we would first sample from,
%
\begin{IEEEeqnarray}{rCl}
 \ed(\anset{\ti}\pss{\notai{\ti}}, \lsset{\ti}\pss{\notai{\ti}} | \pr, \anset{2:\ti-1}, \lsset{1:\ti-1}, \an{\ti:\timax}\pss{\ai{\ti:\timax}}, \ls{\ti:\timax}\pss{\ai{\ti:\timax}}, \aifinal) \nonumber     ,
\end{IEEEeqnarray}
%
and then from,
%
\begin{IEEEeqnarray}{rCl}
 \ed(\an{\ti}\pss{\ai{\ti}}, \ls{\ti}\pss{\ai{\ti}} | \pr, \anset{2:\ti-1}, \lsset{1:\ti-1}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \ls{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \aifinal) \nonumber     .
\end{IEEEeqnarray}

\section{Simulations}

\subsection{The Model}
Particle Gibbs (PG), Particle Gibbs with Backward Simulation (PG-BS) and Particle Gibbs with Refreshed Backward Simulation (PG-RBS) were tested on a tracking model. The transition model is 3D near constant velocity motion \cite{Li2003}, and the observation model is noisy measurements of bearing, elevation and range. The parameter to be learned is the scale factor on the transition covariance matrix $\sigma^2$, which characterises the target manoeuvrability. Data sets are simulated from the model, each with 100 time steps. An uninformative conjugate prior is used for $\sigma^2$.

\subsection{Algorithm Settings}
The algorithms were each run on $5$ different simulated data sets. Each algorithm was run for 5000 iterations, with a burn in of 1000. PG and PG-BS were run twice, with 100 and 200 particles each. PG-RBS was run with 100 particles. PG-RBS with 100 particles takes roughly the same time as PG-BS with 200 particles.

The particle filter uses an extended Kalman filter approximation to the optimal importance density,
%
\begin{IEEEeqnarray}{rCl}
 \id{\ti}(\ls{\ti}|\ls{\ti-1}) & \approx & \den(\ls{\ti}|\ls{\ti-1},\ob{\ti}) \nonumber     .
\end{IEEEeqnarray}
%
PG-RBS is implemented with the CIS method using a similar Gaussian approximation for the importance density,
%
\begin{IEEEeqnarray}{rCl}
 \spd{\ti}(\ls{\ti}|\ls{\ti-1}) & \approx & \den(\ls{\ti}|\ls{\ti-1},\ls{\ti+1},\ob{\ti}) \nonumber     .
\end{IEEEeqnarray}
%
For the proposal weights, we use $\ppw{\ti}\pss{i}=\pw{\ti}\pss{i}$.


\subsection{Results}
PG does not mix well at all. Parameter estimates do not approach the true value, even after 5000 iterations. PG-BS and PG-RBS do converge. Figure~\ref{fig:sample_hist} shows the posterior histograms and figure~\ref{fig:acf} the mean autocorrelation function over the $5$ simulations. The latter indicates faster mixing from PG-RBS with both equal-time and equal-particle equivalents.

\begin{figure}
\centering
\subfloat[PG-BS (N=100)]{ \input{PGtests_2_100_hist.tikz} }
\subfloat[PG-BS (N=200)]{ \input{PGtests_2_200_hist.tikz} }
\subfloat[PG-RBS (N=100)]{ \input{PGtests_3_100_hist.tikz} }
\caption{Posterior sample histograms.}
\label{fig:sample_hist}
\end{figure}

\begin{figure}
\centering
\input{mean_acf_plot.tikz}
\caption{Mean autocorrelation function for PG-BS and PG-RBS.}
\label{fig:acf}
\end{figure}


\section{Discussion and Conclusions}

We have introduced a simple but effective modification to particle Gibbs which can significantly improve the mixing of the Markov chains. This uses a sweep of refreshed backward simulation after each iteration of the conditional particle filter, in which new values of the states of the reference trajectory are sampled along with their corresponding ancestor indexes. Direct sampling of the appropriate conditional distributions is not generally possible, however suitable Markov kernels are available instead. As well as basic Metropolis-Hastings, we have presented a kernel based on conditional importance sampling. The benefit of using refreshed backward simulation will depend on the efficiency of this Markov kernel. Simulations on a simple tracking model have demonstrated substantial improvements in Markov chain autocorrelation. 



% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
\vfill
\pagebreak

\bibliographystyle{IEEEbib}
\bibliography{/users/pete/Dropbox/PhD/Cleanbib}

\end{document}
