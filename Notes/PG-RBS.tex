\documentclass{article}

\usepackage{amsmath}
\usepackage{IEEEtrantools}


\usepackage[scaled]{helvet}\renewcommand*\familydefault{\sfdefault}\usepackage[T1]{fontenc}
\usepackage{setspace}\onehalfspacing


\newenvironment{meta}[0]{\color{red} \em}{}


\title{Particle Gibbs with Refreshed Backward Simulation}
\author{Pete Bunch}
\date{March 2014}

%%% MACROS %%%
\newcommand{\ti}{t}
\newcommand{\timax}{T}
\newcommand{\param}{\theta}
\newcommand{\ls}[1]{x_{#1}}
\newcommand{\ob}[1]{y_{#1}}
\newcommand{\an}[1]{a_{#1}}
\newcommand{\ai}[1]{b_{#1}}
\newcommand{\notai}[1]{-b_{#1}}
\newcommand{\aifinal}{K}
\newcommand{\lsset}[1]{\mathbf{x}_{#1}}
\newcommand{\anset}[1]{\mathbf{a}_{#1}}
\newcommand{\den}{p}
\newcommand{\ed}{\pi}
\newcommand{\td}{f_{\theta}}
\newcommand{\od}{g_{\theta}}
\newcommand{\pd}{q}
\newcommand{\spd}{\rho}
\newcommand{\pw}[1]{w_{#1}}
\newcommand{\pss}[1]{^{(#1)}}
\newcommand{\nump}{N}


\begin{document}

\section{Extended Target Distribution}

The particle MCMC extended target distribution is,
%
\begin{IEEEeqnarray}{rCl}
 \ed(\param, \lsset{1:\timax}, \anset{2:\timax}, \aifinal) & = & \frac{1}{\nump^\timax} \den(\param, \ls{1:\timax}\pss{\ai{1:\timax}}|\ob{1:\timax}) \nonumber \\
  & & \qquad  \times \prod_{i\ne\ai{1}} \pd(\ls{1}\pss{i}) \prod_{\ti=1}^{\timax} \left[ \prod_{i\ne\ai{\ti}} \frac{ \pw{\ti-1}\pss{\an{\ti}\pss{i}} }{ \sum_j \pw{\ti-1}\pss{j} } \pd(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}}) \right] \label{eq:extended_dist_v1}     .
\end{IEEEeqnarray}
%
The necessary rearrangement of this distribution is achieved by expanding the posterior term using Bayes rule,
%
\begin{IEEEeqnarray}{rCl}
 \den(\param, \ls{1:\timax}\pss{\ai{1:\timax}}|\ob{1:\timax}) & = & \frac{ \den(\ob{1:\timax} | \ls{1:\timax}\pss{\ai{1:\timax}}, \param) \den(\ls{1:\timax}\pss{\ai{1:\timax}}|\param) \den(\param|\ob{1:\timax}) }{ \den(\ob{1:\timax}|\param) } \nonumber \\
 & = & \frac{\den(\param|\ob{1:\timax})}{\den(\ob{1:\timax}|\param)} \td(\ls{1}\pss{\ai{1}}) \od(\ob{1}|\ls{1}\pss{\ai{1}}) \prod \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}) \od(\ob{\ti}|\ls{\ti}\pss{\ai{\ti}}) \nonumber       ,
\end{IEEEeqnarray}
%
and then repeatedly applying the following identity,
%
\begin{IEEEeqnarray}{rCl}
 \td(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}}) \od(\ob{\ti}|\ls{\ti}\pss{i}) & = & \pw{\ti}\pss{i} \pd(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}}) \nonumber \\
 \td(\ls{1}\pss{i}) \od(\ob{1}|\ls{1}\pss{i}) & = & \pw{1}\pss{i} \pd(\ls{1}\pss{i}) \label{eq:pmcmc_id}   .
\end{IEEEeqnarray}
%
This leads us to,
%
\begin{IEEEeqnarray}{rCl}
 \ed(\param, \lsset{1:\timax}, \anset{2:\timax}, \aifinal) & = & \frac{\den(\param|\ob{1:\timax})}{\nump^\timax} \frac{\sum_j \pw{\timax}\pss{j}}{\den(\ob{1:\timax}|\param)} \nonumber \\
 & & \times \prod_{i} \pd(\ls{1}\pss{i}) \prod_{\ti=1}^{\timax} \left[ \prod_{i} \frac{ \pw{\ti-1}\pss{\an{\ti}\pss{i}} }{ \sum_j \pw{\ti-1}\pss{j} } \pd(\ls{\ti}\pss{i}|\ls{\ti-1}\pss{\an{\ti}\pss{i}}) \right] \nonumber \\
 & & \times \frac{ \pw{\timax}\pss{\aifinal} }{ \sum_j \pw{\timax}\pss{j} } \label{eq:extended_dist_v2}     .
\end{IEEEeqnarray}
%
This is the form we need for particle marginal Metropolis-Hastings.



\section{Particle Gibbs with Backward Simulation}

In a basic particle Gibbs sampler, we alternately sample $\aifinal$ and $\lsset{1:\timax}\pss{\notai{1:\timax}}, \anset{2:\timax}\pss{\notai{1:\timax}}$. When doing ordinary backward simulation, we add an additional set of steps. Passing backwards through time from $\ti=\timax$ to $\ti=1$, we sample from,
%
\begin{IEEEeqnarray}{rCl}
 \ed(\an{\ti}\pss{\ai{\ti}} | \param, \lsset{1:\ti-1}, \anset{1:\ti-1}, \ls{\ti:\timax}\pss{\ai{\ti:\timax}}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \aifinal)      .
\end{IEEEeqnarray}
%
This is a collapsed Gibbs move. We need to manipulate the extended target distribution as follows. Starting with \eqref{eq:extended_dist_v1}, first marginalise the future variables by alternately integrating out the final state and then summing over the final ancestor indexes. Then expand the posterior term using Bayes rule and use \eqref{eq:pmcmc_id} repeatedly again,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \ed(\param, \lsset{1:\ti-1}, \anset{2:\ti-1}, \ls{\ti:\timax}\pss{\ai{\ti:\timax}}, \an{\ti:\timax}\pss{\ai{\ti:\timax}}, \aifinal) } \nonumber \\
 \qquad \qquad & = & \frac{\den(\param|\ob{1:\timax})}{\nump^{\timax}} \den(\ls{1:\timax}\pss{\ai{1:\timax}}|\ob{1:\timax},\param) \nonumber \\
  & & \times \prod_{i\ne\ai{1}} \pd(\ls{1}\pss{i}) \prod_{k=1}^{\ti-1} \left[ \prod_{i\ne\ai{k}} \frac{ \pw{k-1}\pss{\an{k}\pss{i}} }{ \sum_j \pw{k-1}\pss{j} } \pd(\ls{k}\pss{i}|\ls{k-1}\pss{\an{k}\pss{i}}) \right] \nonumber \\
 & = & \frac{\den(\param|\ob{1:\timax})}{\nump^{\timax}} \frac{ \den(\ob{\ti:\timax}|\ls{\ti:\timax}\pss{\ai{\ti:\timax}},\param) }{ \den(\ob{1:\timax}|\param) } \nonumber \\
 & & \times \prod_{i} \pd(\ls{1}\pss{i}) \prod_{k=1}^{\ti-1} \left[ \prod_{i} \frac{ \pw{k-1}\pss{\an{k}\pss{i}} }{ \sum_j \pw{k-1}\pss{j} } \pd(\ls{k}\pss{i}|\ls{k-1}\pss{\an{k}\pss{i}}) \right] \nonumber \\
 & & \times \pw{\ti}\pss{\an{\ti}\pss{\ai{\ti}}} \times \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}) \nonumber      .  
\end{IEEEeqnarray}
%
Then marginalise $\an{\ti}\pss{\ai{\ti}}$,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \ed(\param, \lsset{1:\ti-1}, \anset{2:\ti-1}, \ls{\ti:\timax}\pss{\ai{\ti:\timax}}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \aifinal) } \nonumber \\
 \qquad \qquad & = & \frac{\den(\param|\ob{1:\timax})}{\nump^{\timax}} \frac{ \den(\ob{\ti:\timax}|\ls{\ti:\timax}\pss{\ai{\ti:\timax}},\param) }{ \den(\ob{1:\timax}|\param) } \nonumber \\
 & & \times \prod_{i} \pd(\ls{1}\pss{i}) \prod_{k=1}^{\ti-1} \left[ \prod_{i} \frac{ \pw{k-1}\pss{\an{k}\pss{i}} }{ \sum_j \pw{k-1}\pss{j} } \pd(\ls{k}\pss{i}|\ls{k-1}\pss{\an{k}\pss{i}}) \right] \nonumber \\
 & & \times \sum_j \pw{\ti}\pss{j} \times \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{j}) \nonumber      ,
\end{IEEEeqnarray}
%
and obtain the conditional,
%
\begin{IEEEeqnarray}{rCl}
 \ed(\an{\ti}\pss{\ai{\ti}} | \param, \lsset{1:\ti-1}, \anset{2:\ti-1}, \ls{\ti:\timax}\pss{\ai{\ti:\timax}}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \aifinal) & = & \frac{ \pw{\ti}\pss{\an{\ti}\pss{\ai{\ti}}} \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}) }{ \sum_j \pw{\ti}\pss{j} \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{j}) }     .
\end{IEEEeqnarray}



\section{Particle Gibbs with Refreshed Backward Simulation}

The change we're going to make is to replace the collapsed Gibbs steps constituting backward simulation with a new set, sampling recursively from,
%
\begin{IEEEeqnarray}{rCl}
 \ed(\ls{\ti}\pss{\ai{\ti}}, \an{\ti}\pss{\ai{\ti}} | \param, \lsset{1:\ti-1}, \anset{1:\ti-1}, \ls{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \aifinal)      .
\end{IEEEeqnarray}
%
The manipulations of the extended target distribution are similar to before,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \ed(\param, \lsset{1:\ti-1}, \anset{1:\ti-1}, \ls{\ti:\timax}\pss{\ai{\ti:\timax}}, \an{\ti:\timax}\pss{\ai{\ti:\timax}}, \aifinal) } \nonumber \\
 \qquad \qquad & = & \frac{\den(\param|\ob{1:\timax})}{\nump^{\timax}} \den(\ls{1:\timax}\pss{\ai{1:\timax}}|\ob{1:\timax},\param) \nonumber \\
  & & \times \prod_{i\ne\ai{1}} \pd(\ls{1}\pss{i}) \prod_{k=1}^{\ti-1} \left[ \prod_{i\ne\ai{k}} \frac{ \pw{k-1}\pss{\an{k}\pss{i}} }{ \sum_j \pw{k-1}\pss{j} } \pd(\ls{k}\pss{i}|\ls{k-1}\pss{\an{k}\pss{i}}) \right] \nonumber \\
 & = & \frac{\den(\param|\ob{1:\timax})}{\nump^{\timax}} \frac{ \den(\ob{\ti+1:\timax}|\ls{\ti+1:\timax}\pss{\ai{\ti+1:\timax}},\param) }{ \den(\ob{1:\timax}|\param) } \nonumber \\
 & & \times \prod_{i} \pd(\ls{1}\pss{i}) \prod_{k=1}^{\ti-1} \left[ \prod_{i} \frac{ \pw{k-1}\pss{\an{k}\pss{i}} }{ \sum_j \pw{k-1}\pss{j} } \pd(\ls{k}\pss{i}|\ls{k-1}\pss{\an{k}\pss{i}}) \right] \nonumber \\
 & & \times \pw{\ti}\pss{\an{\ti}\pss{\ai{\ti}}} \times \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}) \od(\ob{\ti}|\ls{\ti}\pss{\ai{\ti}}) \td(\ls{\ti+1}\pss{\ai{\ti+1}}|\ls{\ti}\pss{\an{\ti+1}\pss{\ai{\ti+1}}}) \nonumber      .
\end{IEEEeqnarray}
%
Now marginalise both $\ls{\ti}\pss{\ai{\ti}}$ and $\an{\ti}\pss{\ai{\ti}}$,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \ed(\param, \lsset{1:\ti-1}, \anset{1:\ti-1}, \ls{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \aifinal) } \nonumber \\
 \qquad \qquad & = & \frac{\den(\param|\ob{1:\timax})}{\nump^{\timax}} \frac{ \den(\ob{\ti+1:\timax}|\ls{\ti+1:\timax}\pss{\ai{\ti+1:\timax}},\param) }{ \den(\ob{1:\timax}|\param) } \nonumber \\
 & & \times \prod_{i} \pd(\ls{1}\pss{i}) \prod_{k=1}^{\ti-1} \left[ \prod_{i} \frac{ \pw{k-1}\pss{\an{k}\pss{i}} }{ \sum_j \pw{k-1}\pss{j} } \pd(\ls{k}\pss{i}|\ls{k-1}\pss{\an{k}\pss{i}}) \right] \nonumber \\
 & & \times \sum_j \pw{\ti}\pss{j} \int \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{j}) \od(\ob{\ti}|\ls{\ti}\pss{\ai{\ti}}) \td(\ls{\ti+1}\pss{\ai{\ti+1}}|\ls{\ti}\pss{\ai{\ti}}) d\ls{\ti}\pss{\ai{\ti}} \nonumber      ,
\end{IEEEeqnarray}
%
and obtain the conditional,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \ed(\ls{\ti}\pss{\ai{\ti}}, \an{\ti}\pss{\ai{\ti}} | \param, \lsset{1:\ti-1}, \anset{1:\ti-1}, \ls{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \an{\ti+1:\timax}\pss{\ai{\ti+1:\timax}}, \aifinal) } \nonumber \\
 \qquad \qquad & = & \frac{ \pw{\ti}\pss{\an{\ti}\pss{\ai{\ti}}} \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}) \od(\ob{\ti}|\ls{\ti}\pss{\ai{\ti}}) \td(\ls{\ti+1}\pss{\ai{\ti+1}}|\ls{\ti}\pss{\ai{\ti}}) }{ \sum_j \pw{\ti}\pss{j} \int \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{j}) \od(\ob{\ti}|\ls{\ti}\pss{\ai{\ti}}) \td(\ls{\ti+1}\pss{\ai{\ti+1}}|\ls{\ti}\pss{\ai{\ti}}) d\ls{\ti}\pss{\ai{\ti}} } \nonumber      .
\end{IEEEeqnarray}

Now the obvious problem is that we cannot sample from this directly. Instead we can use Metropolis-within-Gibbs, using the proposal distribution,
%
\begin{IEEEeqnarray}{rCl}
 \frac{ \pw{\ti}\pss{\an{\ti}\pss{\ai{\ti}}} }{ \sum_j \pw{\ti}\pss{j} } \spd(\ls{\ti}\pss{\ai{\ti}} | \ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}, \ls{\ti+1}\pss{\ai{\ti+1}}, \ob{\ti}) \nonumber      .
\end{IEEEeqnarray}
%
The target:proposal ratio is then proportional to,
%
\begin{IEEEeqnarray}{rCl}
 \frac{ \td(\ls{\ti}\pss{\ai{\ti}}|\ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}) \od(\ob{\ti}|\ls{\ti}\pss{\ai{\ti}}) \td(\ls{\ti+1}\pss{\ai{\ti+1}}|\ls{\ti}\pss{\ai{\ti}}) }{ \spd(\ls{\ti}\pss{\ai{\ti}} | \ls{\ti-1}\pss{\an{\ti}\pss{\ai{\ti}}}, \ls{\ti+1}\pss{\ai{\ti+1}}, \ob{\ti}) }     .
\end{IEEEeqnarray}
%
The acceptance ratio comprises this quantity for the current and proposed values of $(\ls{\ti}\pss{\ai{\ti}}, \an{\ti}\pss{\ai{\ti}})$. The downside to this is that we need to decide how many MH steps to do. Furthermore, if there are only a few values of $\an{\ti}\pss{\ai{\ti}}$ which lead to plausible candidates then this is inefficient.



\section{Conditional Importance Sampling}





\end{document}